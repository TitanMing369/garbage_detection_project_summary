# 大三下大作业第二部分

[TOC]

## 一、诱导大模型总结项目

大模型总结项目和大模型在垃圾桶检测领域的思路扩展，可以有两种路径实现：一种是使用免费的大语言模型文档分析API，直接提交文档分析；还有一种是使用Prompt工程，对各种免费的大模型API，或者在本地部署的大模型（如Ollama）进行诱导，总结项目文章并给出大纲。

使用前者的优点是速度快，效果好，缺点就说未来或者分析需求大时可能收费，涉密资料可能泄露等；后者如果是对自己本地部署的大模型进行Prompt工程的话，优点是灵活度高，可自由调控，不会泄露私密数据，缺点就说有一定算力要求，效果可能也一般。

### 1.Prompt Engingering 介绍

提示工程（Prompt Engingering），也被称为上下文提示（In-Context Prompting），指的是通过**结构化文本等方式来完善提示词，引导LLM输出我们期望的结果。**通过提示词工程可以在不更新模型权重的情况下，让LLM完成不同类型的任务。其主要依赖于经验，而且提示词工程方法的效果在不同的模型中可能会有很大的差异，因此需要大量的实验和探索。

提示工程旨在获取这些提示并帮助模型在其输出中实现高准确度和相关性，掌握提示工程相关技能将有助于用户更好地了解大型语言模型的能力和局限性。特别地， 矢量数据库、agent和prompt pipeline已经被用作在对话中，作为向 LLM 提供相关上下文数据的途径。

提示工程涉及选择、编写和组织提示，以便获得所需的输出，主要包括以下方面：

**Prompt 格式**：确定 prompt 的结构和格式，例如，问题形式、描述形式、关键词形式等。

**Prompt 内容**：选择合适的词语、短语或问题，以确保模型理解用户的意图。

**Prompt 上下文**：考虑前文或上下文信息，以确保模型的回应与先前的对话或情境相关。

**Prompt 编写技巧**：使用清晰、简洁和明了的语言编写 prompt，以准确传达用户的需求。

**Prompt 优化**：在尝试不同 prompt 后，根据结果对 prompt 进行调整和优化，以获得更满意的回应。

提示工程可以帮助改善大语言模型的性能，使其更好地满足用户需求。这是在与模型互动时常用的策略，特别是在自然语言处理任务和生成性任务中，如文本生成、答案生成、文章写作等。

### 2.Prompt Engingering (Advanced prompting to improve reasoning):

#### Zero-shot Prompting

Zero-Shot Prompting 指的是在大型语言模型(LLM)中，不需要额外微调或训练,直接通过文本提示就可以完成指定的下游任务。

主要思想:

先训练一个通用的大型语言模型,学习语言的基本规则,掌握丰富的常识和知识。

然后在不改变模型参数的情况下,只通过软性提示指导模型完成特定任务。

模型根据提示和已掌握的知识,生成对应任务的输出。

缺点：Zero-Shot Prompting 技术依赖于预训练的语言模型，这些模型可能会受到训练数据集的限制和偏见。它的输出有时可能不够准确，或不符合预期。这可能需要对模型进⾏进⼀步的微调或添加更多的提示⽂本来纠正。

#### Few-shot Prompting

few-shot prompting则是通过**提供模型少量高质量的示例，这些示例包括目标任务的输入和期望输出**。通过观察这些良好的示例，模型可以更好地理解人类意图和生成准确输出的标准。

关键思想:

仍然基于预训练好的通用语言模型

使用软提示指导模型完成新任务

额外提供1-2个相关示例作为提示补充

缺点：这种方法可能会消耗更多的token，并且在处理长文本的输入或者输出的时候可能会遇到上下文长度限制的问题。大型语言模型（例如GPT-3）在zero-shot能力方面表现出色。但对于复杂任务，few-shot 提示性能更好。为了提高性能，我们使用，few-shot 提示进行上下文学习，通过在提示中提供演示来指导模型执行任务。换句话说，将模型置于一些特定任务的示例中有助于提高模型性能。

#### Instruction Prompting

指令提示是LLM的最常见用途，尤其是像ChatGPT这样的聊天机器人。指令提示旨在**向大语言模型提供指令提示示例，以便它可以消除训练或测试差异**（模型是在Web规模语料库上训练并在大多数指令上进行测试），**并模拟聊天机器人的实际使用场景**。指令提示**使用（任务说明，输入，真实输出）元组对预训练模型进行微调**，以使模型更好地与用户意图对齐并遵循说明。与说明模型交互时，应该详细描述任务要求，尽量具体和准确，清楚地指定要做什么（而不是说不要做某事）。

#### Chain-of-Thought Prompting

Chain-of-Thought（CoT）提示生成一系列短句，即被称为推理链的句子。思维链提示，就是**把一个多步骤推理问题，分解成很多个中间步骤，分配给更多的计算量，生成更多的 token，再把这些答案拼接在一起进行求解。**对于复杂推理的任务和较大的模型，可获得更多的好处。常见的两种基本CoT提示包括Few-shot CoT 和 Zero-Shot CoT。

Few-shot CoT 允许模型查看一些高质量推理链的演示，这能够帮助模型提高推理能力。普通prompting和CoT的对比如图所示：<img src="md_img\cot.png" alt="cot" style="zoom: 50%;" />

同时，得益于大模型能力的不断提升，CoT也有更简单的版本。Zero-shot CoT是由Kojima等人在2022年首先提出的，他们发现，**只需要在提示中添加  “Let’s think step by step” **，就有助于提高模型性能。让我们看一个下面的例子：<img src="md_img\zero_shot_cot.png" alt="zero_shot_cot" style="zoom:50%;" />

只需要添加一句"Let's think step by step"，LLM会自动将原任务分为几步，一步一步执行，最终得出答案；然后我们可以直接复制LLM的回答，删去答案，作为输入以直接让LLM得到答案。

小对比：

![image-20240809181549729](md_img\image-20240809181549729.png)

#### Self-Consistency CoT

Self-Consistency CoT（自我一致性链式思维提示）是一种改进的解码策略，用于提升链式思维提示（Chain-of-Thought Prompting）的性能。其核心思想是在面对复杂推理任务时，通过生成和评估多种推理路径来找到最一致、最可信的答案，而不是仅仅依赖单一路径的结果（常见的贪婪解码策略：每一步都选择当前最优的选择）。

Self-Consistency CoT 会生成多种不同的推理路径，涵盖了问题的多种思考方式和潜在解决方案；在生成了多种推理路径之后，模型会通过对这些路径的结果进行统计，找出最一致的答案。这个过程类似于“投票”，即看哪种答案在不同推理路径中出现的次数最多，选择这个最具一致性的答案作为最终输出。

![image-20240808193327461](md_img\SCCoT.png)

自我一致性策略显著提升了链式思维提示在一系列流行的算术和常识推理基准测试中的表现，在基准测试中，自我一致性策略的表现提升幅度非常明显，例如GSM8K提升了17.9%、SVAMP提升了11.0%、AQuA提升了12.2%、StrategyQA提升了6.4%，以及ARC挑战提升了3.9%。



#### Tree of Thoughts

思维树（Tree of Thoughts, ToT）是对Chain of Thought 提示方法的推广。

与传统的逐字逐句从左到右进行推理的方式不同，ToT框架允许语言模型同时探索多条不同的推理路径，模型可以在不同的路径之间进行比较和选择，以找到最佳解决方案。同时，ToT将推理过程分解为多个连贯的“思维”单元，其中每个单元都是通向最终解决方案的中间步骤。模型可以在每个步骤上进行自我评估，选择最有前景的路径继续推进。ToT框架允许模型在推理过程中进行前瞻，即考虑未来的可能选择，还可以回溯到之前的步骤，以修正可能的错误或重新评估决策。这种能力大大增强了模型在复杂任务中的灵活性和准确性。

![image-20240808194950412](md_img\ToT.png)

原作者提到，ToT特别适用于那些需要战略性规划、探索或全局性思考的任务。比如，在游戏策略、创意写作和填字游戏等任务中，ToT显著提升了语言模型的表现。例如，在24点游戏中，ToT使模型的成功率从仅4%提升到了74%。

![image-20240809184154411](md_img\image-20240809184154411.png)

<img src="md_img\image-20240809185915406.png" alt="image-20240809185915406" style="zoom: 200%;" />

此外，像ToT这样的搜索方法需要更多的资源（如GPT-4 API成本）来提高任务性能，但ToT的模块化灵活性让用户可以自定义这种性能-成本平衡。

#### ReAct

近年来的进展扩展了语言模型（LM）在下游任务中的适用性。一方面，通过链式思维提示，现有的语言模型展示了新的能力，能够通过自我条件化的推理轨迹从问题中得出答案，在各种算术、常识和符号推理任务中表现出色。然而，使用链式思维提示时，模型并未与外部世界建立联系，而是使用其内部表示来生成推理轨迹，这限制了模型在反应性探索和推理以及更新知识方面的能力。另一方面，近期的研究利用预训练语言模型在各种交互环境中进行规划和行动（如文字游戏、网页导航、具象任务、机器人领域），重点是通过语言模型的内部知识将文本上下文映射到文本行为。然而，这些方法并未对高层次目标进行抽象推理，也没有维持工作记忆来支持长时间的行动。

ReAct（Synergizing Reasoning and Acting in Language Models）是一种通用范式，将**推理与行动的进展结合起来，使语言模型能够解决各种语言推理和决策任务**。原文中展示了在提示更大的语言模型和微调较小的语言模型时，“推理+行动”（ReAct）范式系统性地优于仅推理或仅行动的范式。推理与行动的紧密结合还展示了与人类一致的任务解决路径，提高了模型的可解释性、诊断性和可控性。

ReAct 使语言模型能够以交替的方式生成语言推理轨迹和文本行动。虽然行动会导致来自外部环境（图中“Env”）的观察反馈，但推理轨迹不会影响外部环境。相反，它们通过对上下文进行推理，并用有用的信息更新模型的内部状态，从而支持未来的推理和行动。

<img src="md_img\ReAct.png" alt="image-20240808211438149" style="zoom:50%;" />

![image-20240809191136593](md_img\image-20240809191136593.png)

原作者在四个不同的基准测试中对ReAct和最先进的基线方法进行了实证评估：问答（HotPotQA）、事实验证（Fever）、基于文本的游戏（ALFWorld）和网页导航（WebShop）。对于HotPotQA和Fever，模型可以通过与Wikipedia API交互，ReAct在生成行动模型方面优于基础的生成模型，同时在链式推理（CoT）性能上具有竞争力。最佳结果的方法是将ReAct与CoT相结合，在推理过程中同时利用内部知识和外部获取的信息。在ALFWorld和WebShop上，使用单次提示和两次提示的ReAct分别在成功率上比通过约105个任务实例训练的模仿学习和强化学习方法高出34%和10%的绝对提升，超越了现有的基线方法。

#### Recursive Prompting

递归提示是一种问题解决方法，它涉及**将复杂问题分解成更小、更易管理的子问题**，然后通过一系列提示递归地解决这些子问题。这种方法对需要组合泛化的任务尤其有用，其中语言模型必须学习如何组合不同的信息来解决问题。

在自然语言处理的背景下，递归提示可以使用少量提示方法将复杂问题分解为子问题，然后顺序解决提取的子问题，使用前一个子问题的解决方案来回答下一个子问题。这种方法可以用于数学问题或问答等任务，其中语言模型需要能够将复杂问题分解为更小、更易管理的部分，以得出解决方案。复杂问题的分解目前主要可以分为两列：并行式和串行式

（1）**阅读理解场景下**，**多跳阅读理解要求从众多段落中进行推理跟归纳**。于是出现了新的方案，将多跳阅读理解问题分解成多个相对简单的子问题（现有阅读理解模型可以回复），从而提高阅读理解准确性。如图：

<img src="md_img\parall_recursing.png" alt="parall_recursing" style="zoom:50%;" />

回答原始问题Q综合需要两个不同段落的信息，而将问题拆解为子问题Q1和Q2，能显著提高模型的回答准确率。

整个方案分为三个部分

​     a) **将原始的多跳阅读理解问题分解为多个单跳子问题。可以根据多个不同的推理类型得到多种分解方式**，这里需要根据不同推理类型分别训练多个用于问题分解的模型，对于每个分解模型，采用Point的方式，利用BERT对原问题进行预测，得到几个关键位置，利用关键位置原文本进行划分，再加上一些规则手段，就可以得到对应的子问题了。例如预测出一个中间位置，就可以将原问题分割成两部分，第一部分作为第一个子问题，第二部分作为第二个子问题，考虑到第二部分可能都是陈述句，就将前面的词转换成which。这里将分解模型简化为一个span prediction问题，只需要400个训练数据就得到很不错的效果了。

​     b) 在第一步会产生多种问题分解方式，**对于每一种分解方式，利用单跳阅读理解模型回复每个子问题，然后根据不同分分解类型的特性得到最终的答案。**

​     c) 对于每一种分解方式，将原问题，分解类型，该分解方式下的问题跟对应答案一同作为模型输入，预测哪种分解方式对应的结果最合理，将该分解方式下的答案作为多跳阅读理解问题的答案。

（2）**在QA场景下，通过将复杂问题分解为相对简单的子问题（QA模型可以回复）**，从而提高问答的效果。具体到多跳QA问题上，现将复杂问题分解为多个子问题，利用单跳QA模型生成全部子问题的答案并融合到一起作为复杂问题的答案。

整个系统分为三个部分

​      a) **无监督问题分解，将原问题分解为多个相对简单的子问题**。这里需要训练一个分解模型，用于将复杂问题分解成多个子问题。由于这个任务下的监督训练数据构造成本高昂，于是提出了一种无监督的训练数据构造方式，对于每一个复杂问题q，从语料集Q中检索召回得到N个对应的简单问题s作为q的子问题，N的取值可以依赖于具体任务或者具体问题。我们希望这些简单问题在某些方面跟q足够相似，同时这些简单问题s之间有明显差异。从而构造出复杂问题跟子问题序列之间的伪pair对（q, [s1,…sN]）,用于训练分解模型。

<img src="md_img\divide_q.png" alt="divide_q" style="zoom: 50%;" />

​     b) 生成**子问题回复，利用现有的QA模型，去生成各个子问题的回复**。这里不对QA模型有太多限制，只要它能正确回复语料库S中的简单问题即可，所以尽量采用在S中效果优异的QA模型。

​     c) **生成复杂问题回复，将复杂问题，各个子问题跟对应回复一同作为QA模型的输入，生成复杂问题的回复。**这里的QA模型可以采用跟第二步一样的模型，只要将输入做对应调整即可。

<img src="md_img\divided_question.png" alt="divided_question" style="zoom: 80%;" />

 （3）**如何利用大规模语言模型将自然语言问题转化为代码语句？SEQZERO就是一种解法。**以SQL语句为例，由于SQL这种规范语言的复合结构，SQL语句很多情况下会显得复杂且冗长。  **一个SQL语句包括多个部分，例如From \**，SELCT \**， WHERE \**，只要能从自然语言问题中提出这几个部分对应的元素，然后通过规则可以转化为对应的SQL语句。**

**于是SEQZERO的做法就是先利用语言模型预测得到其中一个元素，将该元素加入到原问题中生成下个元素，重复此操作直到生成全部元素，然后通过规则将所有结果组合起来的就得到对应的SQL语句。**在预测每个元素的过程中，为了得到更加强大的泛化能力，采用了few-shot跟zero-shot的集成策略。

![SQL](md_img\SQL.png)

（4）**Least-to-most**

**虽然chain-of-thought prompting在很多自然语言推理任务有显著效果，但是当问题比prompt里的示例更难时，它的表现会很糟糕。**举个例子，比如任务抽取文本每个单词最后一个字母，prompt的示例输入是3个单词，输入相对较短，但是问题的长度却是10个单词，这种情况下chain-of-thought prompting的策略就会失效。于是提出了**Least-to-most，通过两阶段的prompting来解决这种问题，第一阶段通过prompting将原问题分解为一系列子问题，第二阶段则是通过prompting依次解决子问题，**前面子问题的问题跟答案会加入到候选子问题的模型输入中去，方便语言模型更好地回复候选子问题。由于这两个阶段任务有所区别，对应的prompt内容也不同。

<img src="md_img\least2most.png" alt="least2most.png" style="zoom: 80%;" />

#### CoT 与 Recursive Prompting的混合使用

自我提问（self-ask ）可以视为另一种类型的递归提示，是一种反复提示模型提出后续问题以迭代构建思维过程的方法。后续问题可以通过搜索引擎结果来回答。类似地，IRCoT 和 ReAct将迭代 CoT 提示与对 Wikipedia API 的查询相结合，以搜索相关实体和内容，然后将其添加回上下文中。

#### Automatic Prompt Engineer

Automatic Prompt Engineer (APE) 是一种自动化方法，用于生成和优化输入提示（prompts），以最大化大型语言模型（LLMs）在特定任务上的性能。其目标在于减少对人类提示工程师的依赖，通过自动生成高质量的提示来提高模型的输出效果。

APE将指令视为“程序”，通过在LLM提出的一组候选指令中进行搜索，优化以最大化选定的评分函数。

![image-20240808201643430](md_img\APE.png)

APE 还会通过迭代过程优化这些提示。这个过程通常包括评估模型输出的质量，并基于输出结果调整提示，从而逐步提高模型的表现。APE生成的提示不仅可以引导模型向真实性和/或信息性方向发展，还可以通过简单地将其添加到in-context learning prompts前来改善few-shot learning的表现。

#### 更多方法

##### Reprompting

Reprompting是一种迭代采样算法，能够在无需人工干预的情况下自动学习针对特定任务的链式思维（Chain-of-Thought, CoT）方案。通过吉布斯采样（Gibbs Sampling），Reprompting 通过迭代采样新的方案，并利用之前采样的方案作为“父”提示来解决其他训练问题，从而推断出对一组训练样本始终表现良好的CoT方案。

##### STaR

Self-Taught Reasoner（STaR）可以迭代利用少量推理示例和一个没有推理过程的大型数据集，以逐步增强模型执行更复杂推理的能力。基于一个简单的循环：通过少量推理示例提示生成推理过程以回答大量问题；如果生成的答案错误，则尝试给定正确答案再次生成推理过程；对最终产生正确答案的所有推理过程进行微调；重复此过程。

##### Reflexion

Reflexion是一种新颖的框架，通过语言反馈来强化语言智能体，而不是通过更新权重。具体来说，Reflexion智能体会对任务反馈信号进行语言反思，然后将这些反思性文本保存在情景记忆缓冲区中，以在后续试验中促使更好的决策。Reflexion足够灵活，能够整合各种类型（标量值或自由形式的语言）和来源（外部或内部模拟）的反馈信号，并在各类任务（顺序决策、编码、语言推理）中显著超过基线智能体。

##### DSP框架

检索增强的上下文学习（Retrieval-augmented in-context learning）通过使用冻结的语言模型（LM）和检索模型（RM）来实现。目前的研究通常将这些模型简单地结合在"retrieve-then-read"的流水线中，其中RM检索出相关段落，并将其插入到LM的提示中。为了充分发挥冻结LM和RM的潜力，Demonstrate-Search-Predict（DSP）框架被提出。该框架依赖于在LM和RM之间通过复杂的流水线传递自然语言文本。DSP能够表达高级程序，启动面向流水线的示范，搜索相关段落，并生成有依据的预测，从而系统地将问题分解为LM和RM能够更可靠处理的小变换。

##### Multiagent Debate方法

Multiagent Debate使一种来改进语言响应的互补方法，即多个语言模型实例提出各自的响应和推理过程，并通过多轮讨论和辩论来达成共同的最终答案。这种方法显著增强了在多个任务中的数学和策略推理能力。我们还证明了该方法能够提高生成内容的事实准确性，减少现有模型易犯的错误和幻觉。该方法可以直接应用于现有的黑箱模型，并在我们研究的所有任务中使用相同的程序和提示。

<img src="md_img\MultiagentDebate.png" alt="image-20240808210504681" style="zoom:50%;" />

##### Faithful Reasoning方法

Faithful Reasoning方法通过一个反映问题潜在逻辑结构的因果关系过程，使LMs能够进行可靠的多步推理。该方法通过将推理步骤链接在一起实现，每个步骤由两个经过微调的模型调用得出，一个用于选择，另一个用于推理，以生成有效的推理轨迹。同时Faithful Reasoning也会执行束搜索来提高推理质量。

<img src="md_img\FaithfulReasoning.png" alt="image-20240808210850634" style="zoom: 80%;" />



### 3.大模型总结项目的具体实现

#### 1.使用本地部署的大模型，用Prompt工程诱导总结项目

我们使用的是Ollama+Open-webui平台跑通本地大语言模型的，具体搭建流程可参考：[Ollama+Open WebUI本地部署Llama3 8b（附踩坑细节）-CSDN博客](https://blog.csdn.net/qq_53795212/article/details/139690567)

Ollama社区提供有多种前沿大语言模型可供下载，调试。我们使用了 270 亿参数的gemma2:27大模型来总结项目。Gemma是Google推出的一系列轻量级、最先进的开放模型，基于创建Gemini模型的相同研究和技术构建。Gemma所有版本均可在各类消费级硬件上运行，无需数据量化处理，使得我们可以在普通性能笔记本上部署和运行它。上下文长度为8192个token，使得模型可以轻松总结我们的含有大量token的项目描述。同时，我们选择的Gemma2:27是目前Gemma系列参数量最大，最新的版本。

我们使用的Prompt方法主要是前面介绍过的CoT 与 递归提示的混合使用：辅助模型拆解问题，形成思维链；以及将复杂问题拆解为一个一个小问题，方便模型详尽全面的解释垃圾桶检测项目：

<img src="md_img\gemma.png" alt="image-20240807151430302" style="zoom:50%;" />

一步一步总结完项目后，我们总结选择PDF或者TXT格式下载整个对话：

![image-20240807151626646](md_img\download_gemma.png)

#### 2.使用在线大模型平台总结项目

这次大作业我们使用在线平台的主要是ChatGPT，优点是有可以称为最强在线大模型GPT4使用，缺点是有限额并且不能直接下载对话。

我们使用的Prompt方法主要是前面介绍过的**CoT 与 递归提示的混合使用：辅助模型拆解问题，形成思维链**；以及将复杂问题拆解为一个一个小问题，方便模型详尽全面的解释垃圾桶检测项目：

<img src="md_img\GPT.png" alt="image-20240807152108786" style="zoom:50%;" />

由于GPT没有提供下载功能，我们直接复制对话内容保存为TXT。

#### 3.使用免费的大模型文档处理平台总结项目

这里以Humata平台为例，Humata本身**是基于大语言模型，辅助阅读论文的一个平台。**它同时也能对一个，或者多个PDF文档进行总结，非常契合本次任务要求。

![image-20240730181254914](md_img\humata.png)

只需要注册，登录，上传文档，然后就可以提问有关这些文档的问题了。

<img src="md_img\humata_ask.png" alt="humata_ask" style="zoom:75%;" />

例：让humata总结Llama2论文，并给出20页PPT方案。

<img src="md_img\show_humata.png" alt="image-20240730181915899" style="zoom:50%;" />

在使用以上几种混合方法，从头到尾生成PPT内容方案后，我们优中选优，把各个大模型的总结汇聚一起形成文档，用文档引导大模型做一个演讲稿总结，如果第一遍总结的不到位，我们可以继续Prompt提示它应该总结的方向，最终得到我们想要的演讲稿。之后就进入到使用文档和演讲稿生成PPT的步骤。

### 4.以ChatGPT生成AI讲稿为例

#### 提示工程的基本策略(OpenAI指南)

我们之前介绍的Prompt Engingering 方法偏向理论，其中甚至有些方法是为了给予开发者参考使用的。对于用户端在使用ChatGPT时需要的简明实用prompt技巧，OpenAI在其官方文档中给出了指导指南：

[[Prompt engineering](https://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering)](https://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering)

本指南分享了一些策略和战术，以帮助您从大型语言模型（有时称为GPT模型，如GPT-4o）中获得更好的结果。这里描述的方法有时可以结合使用以达到更好的效果。我们鼓励您进行实验，找到最适合自己的方法。您还可以探索一些示例提示，展示我们的模型的能力：

**获得更好结果的六大策略**

**1.编写清晰的指令**

这些模型无法读取您的想法。如果输出过长，要求简短的回复。如果输出过于简单，要求专家级的写作。如果您不喜欢输出的格式，展示您想要的格式。模型需要猜测您需求的部分越少，您获得理想结果的可能性就越大。 战术：

- 在查询中包含细节以获得更相关的答案
- 要求模型采用特定角色
- 使用分隔符清楚地标明输入的不同部分
- 指定完成任务所需的步骤
- 提供示例
- 指定所需的输出长度

**2.提供参考文本**

语言模型可能会自信地编造错误答案，尤其是当被问及冷僻话题或引文和URL时。就像一张笔记可以帮助学生在考试中表现更好一样，向这些模型提供参考文本可以帮助减少虚构内容。 战术：

- 指导模型使用参考文本作答
- 指导模型使用参考文本中的引文作答

**3.将复杂任务拆分为更简单的子任务**

就像在软件工程中将复杂系统分解为一组模块化组件一样，提交给语言模型的任务也应如此。复杂任务的错误率往往比简单任务高。此外，复杂任务通常可以重新定义为简单任务的工作流程，其中早期任务的输出用于构建后续任务的输入。 战术：

- 使用意图分类以识别用户查询中最相关的指令
- 对需要非常长对话的对话应用程序，摘要或过滤之前的对话
- 分段摘要长文档并递归构建完整摘要

**4.给模型时间“思考”**

如果让您计算17乘以28，您可能不会立即知道答案，但仍然可以通过时间推算出来。同样，模型在尝试立即回答时容易出现推理错误，而不是花时间去推算出答案。要求在作答前先进行“思维链”可以帮助模型更可靠地推理出正确的答案。 战术：

- 指导模型在做出结论前先推导出自己的解决方案
- 使用内心独白或一系列查询来隐藏模型的推理过程
- 询问模型是否在前几次尝试中遗漏了任何内容

**5.使用外部工具**

通过向模型提供其他工具的输出来弥补其弱点。例如，文本检索系统（有时称为RAG或检索增强生成）可以告知模型相关文档。像OpenAI的代码解释器这样的代码执行引擎可以帮助模型进行数学运算和运行代码。如果某个任务由工具而非语言模型完成会更可靠或更高效，请将其转移给工具，以获得两者的最佳效果。 战术：

- 使用基于嵌入的搜索实现高效的知识检索
- 使用代码执行进行更准确的计算或调用外部API
- 让模型访问特定功能

**6.系统地测试更改**

如果能够测量性能，改进就会变得更容易。在某些情况下，对提示的修改可能会在少数孤立的示例中表现更好，但在更具代表性的示例集上表现更差。因此，为确保更改对性能的净增益，可能需要定义一个全面的测试套件（也称为“评估”）。 战术：

- 使用标准答案评估模型输出

上面列出的每种策略都可以通过具体的战术来实现。这些战术旨在为您提供可尝试的思路。它们并不完全详尽，您可以尝试在此基础上进行创造性的扩展。



##### 策略：编写清晰的指令

**战术：在查询中包含细节以获得更相关的答案**

为了获得高度相关的响应，请确保请求提供所有重要的细节或上下文。否则，您就让模型去猜测您的意思。

**战术：要求模型采用特定角色**

系统消息可以用于指定模型在回复中使用的角色。

**战术：使用分隔符清楚地标明输入的不同部分**

使用分隔符（如三重引号、XML标签、章节标题等）可以帮助区分不同部分的文本，并将其分别处理。

**战术：指定完成任务所需的步骤**

有些任务最好指定为一系列步骤。明确写出步骤可以使模型更容易遵循。

**战术：提供示例**

提供适用于所有示例的一般指令通常比通过示例演示任务的所有变体更有效率，但在某些情况下，提供示例可能更容易。例如，如果您打算让模型复制特定的回应用户查询的风格，而这种风格难以明确描述。这就是所谓的“少样本”提示。

**战术：指定所需的输出长度**

您可以要求模型生成具有目标长度的输出。目标输出长度可以以字数、句子、段落、要点等为单位指定。但请注意，指示模型生成特定字数的效果不如指定特定段落或要点的效果精确。

##### 策略：提供参考文本

**战术：指导模型使用参考文本作答**

如果我们能为模型提供与当前查询相关的可信信息，那么我们可以指导模型使用所提供的信息来撰写答案。

**战术：指导模型使用参考文本中的引文作答**

如果输入已补充了相关知识，那么可以直接要求模型通过引用所提供的文档中的段落为其答案添加引文。请注意，输出中的引文可以通过在提供的文档中进行字符串匹配来程序化地验证。

##### 策略：将复杂任务拆分为更简单的子任务

**战术：使用意图分类以识别用户查询中最相关的指令**

对于需要大量独立指令集来处理不同情况的任务，首先对查询类型进行分类，并使用该分类来确定所需指令可能更有益。可以通过定义固定的类别并对处理给定类别任务所需的指令进行硬编码来实现这一点。这一过程还可以递归应用，将任务分解为一系列阶段。这种方法的优点在于，每个查询只包含执行任务下一阶段所需的指令，与使用单个查询执行整个任务相比，这可以降低错误率。此外，由于较大的提示成本更高，这也可以降低成本。

例如，对于客户服务应用程序，可以对查询进行如下分类：

**战术：对需要非常长对话的对话应用程序，摘要或过滤之前的对话**

由于模型的上下文长度是固定的，因此包含整个对话的上下文窗口中的用户与助手之间的对话不能无限制地继续下去。 这个问题有多种解决方法，其中之一是对话的前几轮进行总结。一旦输入达到预定的阈值长度，这可以触发对部分对话的总结，并将之前对话的总结作为系统消息的一部分包含进来。另一种解决方案是动态选择与当前查询最相关的对话部分。请参见战术“使用基于嵌入的搜索实现高效知识检索”。

**战术：分段摘要长文档并递归构建完整摘要**

由于模型的上下文长度是固定的，不能用单个查询来总结比上下文长度减去生成的摘要长度还长的文本。 为了总结一本非常长的书籍，我们可以使用一系列查询来总结书籍的每个部分。部分摘要可以连接起来并总结，生成摘要的摘要。这个过程可以递归进行，直到总结出整本书籍的内容。如果需要使用早期部分的信息来理解后续部分的内容，那么另一个有用的技巧是包含文本的前述部分的运行摘要，同时总结该点的内容。OpenAI之前的研究使用GPT-3的变体研究了这种总结书籍程序的有效性。

##### 策略：给模型时间“思考”

**战术：指导模型在做出结论前先推导出自己的解决方案**

有时，当我们明确指导模型从第一原理推理得出结论时，我们会得到更好的结果。例如，我们希望模型评估学生对数学问题的解决方案。最明显的方法是简单地询问模型学生的解决方案是否正确。

**战术：使用内心独白或一系列查询来隐藏模型的推理过程**

前一个战术表明，在回答具体问题之前，模型有时需要详细推理问题。对于某些应用程序，模型在得出最终答案时使用的推理过程不应与用户分享。例如，在辅导应用程序中，我们可能希望鼓励学生自己推导出答案，但模型对学生解决方案的推理过程可能会向学生透露答案。 内心独白是一种可以用来缓解这个问题的战术。内心独白的想法是指示模型将输出中不打算展示给用户的部分放入一种结构化的格式中，以便于解析。然后，在展示给用户之前，对输出进行解析，并只展示输出的一部分。

**战术：询问模型是否在前几次尝试中遗漏了任何内容**

假设我们正在使用模型列出某个来源中与特定问题相关的摘录。在列出每个摘录后，模型需要确定它是否应该开始写下一个摘录或是否应该停止。如果源文档很大，模型通常会过早停止并未列出所有相关摘录。在这种情况下，通过后续查询提示模型找到前几次尝试中遗漏的摘录，通常可以获得更好的性能。

##### 策略：使用外部工具

**战术：使用基于嵌入的搜索实现高效的知识检索**

如果作为输入的一部分提供外部信息来源，模型可以利用这些信息生成更有见地和更新的响应。例如，如果用户问及某部特定电影的相关问题，可能需要向模型的输入中添加关于电影的高质量信息（如演员、导演等）。嵌入可以用于实现高效的知识检索，以便在运行时动态地将相关信息添加到模型输入中。 文本嵌入是一种向量，可以衡量文本字符串之间的相关性。相似或相关的字符串将比无关的字符串更接近。这个事实，以及快速向量搜索算法的存在，意味着嵌入可以用来实现高效的知识检索。特别是，可以将文本语料库分割为多个部分，每个部分都嵌入并存储。然后，可以嵌入给定查询并执行向量搜索，以查找与查询最相关的文本部分（即在嵌入空间中最接近的部分）。 在OpenAI Cookbook中可以找到示例实现。请参阅战术“指导模型使用检索到的知识回答查询”了解如何使用知识检索最小化模型捏造错误事实的可能性。

**战术：使用代码执行进行更准确的计算或调用外部API**

语言模型不能仅靠自己进行可靠的算术运算或长时间计算。在需要这种操作的情况下，可以指示模型编写并运行代码，而不是自己计算。特别是，可以指示模型将需要运行的代码放入指定的格式中，例如三重反引号。在生成输出后，可以提取并运行代码。最后，如果有必要，可以将代码执行引擎（如Python解释器）的输出作为输入提供给模型进行下一次查询。

**战术：让模型访问特定功能**

聊天完成API允许在请求中传递功能描述列表。这使得模型能够根据提供的架构生成功能参数。生成的功能参数由API以JSON格式返回，并可以用于执行功能调用。功能调用提供的输出可以在随后的请求中反馈给模型以闭环。这是使用OpenAI模型调用外部功能的推荐方法。有关更多信息，请参阅我们介绍性文本生成指南中的功能调用部分，并在OpenAI Cookbook中查看更多功能调用示例。

##### 策略：系统地测试更改

有时，很难判断某项更改——例如，新指令或新设计——是让您的系统变得更好还是更糟。查看一些示例可能会暗示哪个更好，但样本量小的情况下，很难区分是真正的改进还是随机的运气。也许更改有助于某些输入的性能，但对其他输入的性能却有害。 评估程序（或“评估”）对于优化系统设计非常有用。好的评估：

- 具有现实世界使用的代表性（或至少是多样化的）
- 包含许多测试用例以获得更大的统计效力（请参见下表中的指南）
- 易于自动化或重复 差异检测 样本量要求（以获得95%置信度） 30% ~10 10% ~100 3% ~1,000 1% ~10,000 输出的评估可以由计算机、人类或两者结合完成。计算机可以使用客观标准（例如，具有唯一正确答案的问题）以及一些主观或模糊标准来自动化评估，其中模型输出通过其他模型查询进行评估。OpenAI Evals是一个开源软件框架，提供用于创建自动化评估的工具。 当存在一系列可能的输出都被视为同样高质量的情况时，基于模型的评估可能是有用的（例如，对于长答案的问题）。可以通过基于模型的评估来合理评估什么内容，什么内容需要人类进行评估的界限是模糊的，并且随着模型能力的提高而不断变化。我们鼓励实验以找出基于模型的评估在您的用例中能发挥多大作用。

**战术：使用标准答案评估模型输出**

假设已知问题的正确答案应参考一组已知事实。然后，我们可以使用模型查询来计算答案中包含了多少个所需的事实。



#### 生成AI讲稿

在本次项目的视频中，有一段内AI语音讲解PPT的内容，其中讲稿的文本内容正是由ChatGPT根据PPT内容生成的，下面将会对实际操作过程，包括prompt的过程和中途遇到的部分问题进行介绍。

在我们生成讲稿的时期，OpenAI对于免费用户只开放了GPT-4o和GPT-4o mini模型，其中GPT-4o限额严格，可用对话频率过低，所以我们在这次prompt过程中全程使用的是**GPT-4o mini**。由于免费用户无法上传文件或者图片，所以本次生成过程中我们将PPT每页的文本内容以复制粘贴的形式告诉GPT，图片则加以简单描述告诉GPT。（直接使用PPT文档生成总结的平台也有很多，如Gamma等。但本次生成演讲稿对于文本格式有特殊的要求，同时目前GPT在总结文本的能力较强，我们本次使用的是演讲稿）

首先我们需要明确我们的需求：我们已有一份PPT，需要对于PPT的每一页撰写对应的讲稿。同时由于该讲稿需要给与AI语音生成模型训练，这段文本除了需要具有一般讲稿的形式外，还需要满足以下要求：

- 要求1：出现的英文单词（有发音的完整单词）小写
- 要求2：英文缩写需要根据读音转化为相同读音的英文单词（例如YOLO ==> you low）
- 要求3：讲稿的每段不换行，全部使用相同的字体格式
- 要求4：不可出现空格或中英文括号
- 要求5：数字和数学符号需要使用用汉字读法表达（例如98% ==> 百分之九十八）

##### 开头

在开头我们明确自己的**要求**和**背景**，并将其详细告知GPT：

<img src="md_img\image-20240809155131693.png" alt="image-20240809155131693" style="zoom: 50%;" />

在开头提到的要求中，我们遗漏了一些要求，这是因为有些要求是我们中途使用讲稿中临时发现的，这部分要求我们会在中途需要GPT实现的时候通过对话额外增加。

同时，由于在之前与GPT对话时，GPT出现了我们没有想到的违规方式，所以我们在这次的prompt的开头增加了上一次prompt缺失的要求，例如：“不需要标注每段的段数”；”禁止在每段最后出现重复的总结或者展望“” 一定不要出现我没有提到的技术或方法”。下面是这次的首段与之前未提及” 一定不要出现我没有提到的技术或方法”的首段对比：

![Slide1](md_img\Slide1.PNG)

![Slide2](md_img\Slide2.PNG)



<img src="md_img\image-20240809160103374.png" alt="image-20240809160103374" style="zoom: 50%;" />

**未添加” 一定不要出现我没有提到的技术或方法“：**

<img src="md_img\image-20240809160355199.png" alt="image-20240809160355199" style="zoom: 50%;" />

另外一个问题是，我们在开头所叙述的如此多的要求，GPT能否持续记忆下去？能否持续执行下去？甚至GPT是否在一开始就理解了我们的要求？实际上，记忆对于GPT来说并不难，而后面两点才是我们在中途需要注意并提醒的点。

之后我们继续prompt过程：

<img src="md_img\image-20240809161029369.png" alt="image-20240809161029369" style="zoom:50%;" />

##### 中途提示

在途中的一段中，我们发现对于无关的图片，GPT写讲稿的时候也会使用很多文本解释图片，所以我们在这里即使**添加新的要求**，GPT则会更新他的Memory：

<img src="md_img\image-20240809162934839.png" alt="image-20240809162934839" style="zoom:50%;" />

<img src="md_img\image-20240809162959550.png" alt="image-20240809162959550" style="zoom:50%;" />

我们可以看到这次的鹦鹉图片GPT直接进行了跳过（在之后对话中出现的所有无关图片，GPT也自主进行了跳过）

<img src="md_img\image-20240809163019763.png" alt="image-20240809163019763" style="zoom:50%;" />

这里，我们添加了数字读法的要求并重复了大小写的问题。数字读法因为只靠语言叙述较为困难，我们直接**给出案例**帮助GPT理解，GPT处理的很好。而大小写的问题实际上在后面的对胡中反复出现，这是因为我们一开始的要求中，GPT可能并没有理解到我所说的哪些属于单词（需要全部小写），例如"LoRA""Text-to-Image"，我们在这时需要及时告诉他，这些单词也需要全部小写。

<img src="md_img\image-20240809163109473.png" alt="image-20240809163109473" style="zoom:50%;" />

可以看到GPT对数字的处理应用了新的规则

<img src="md_img\image-20240809163433065.png" alt="image-20240809163433065" style="zoom:50%;" />

在这里，GPT对于大小写和删除括号的方式出现了错误，在此之前，我们已经对于括号进行了一次举例解释：

<img src="md_img\image-20240809164356499.png" alt="image-20240809164356499" style="zoom:50%;" />

<img src="md_img\image-20240809164410686.png" alt="image-20240809164410686" style="zoom:50%;" />

然而这次的HERD-YOLO（Hybrid Energy-efficient ResDense YOLO）GPT并没有相似处理。

<img src="md_img\image-20240809164118675.png" alt="image-20240809164118675" style="zoom:50%;" />

我们直接进行提示

<img src="md_img\image-20240809164145719.png" alt="image-20240809164145719" style="zoom:50%;" />

这里原本的HERD-YOLO在我上一步提出修改大小写的时候并没有变为小写，而在下一步我提示HERD-YOLO需要替换发音时，GPT将其修改成为了小写

<img src="md_img\image-20240809164208804.png" alt="image-20240809164208804" style="zoom:50%;" />

这里也是重复了两次删去括号的要求

<img src="md_img\image-20240809164958718.png" alt="image-20240809164958718" style="zoom:50%;" />

##### 结尾

我们在全部结束后询问GPT是否还记得我们的所有要求：

<img src="md_img\image-20240809165433789.png" alt="image-20240809165433789" style="zoom: 80%;" />

我们可以发现GPT对于一开始的要求和中途添加的要求都记忆牢固。

##### 总结

对于GPT的记忆我们可以发现其基本不会有所遗漏，而执行出现问题时，多数为GPT没有理解我们的要求，包括：”要求本身没有理解“”分类判断不明确，从而对新的目标没有按规则进行处理“，”类比能力低，识别不出新的目标和已有的目标为相同类别“等。

同时我们在提出要求的时候，有可能会出现自我矛盾的地方而我们自己没有意识到，例如对于括号的处理，在本次prompt中GPT对于”删除括号“的方式不知晓是”直接删除“还是”改变句式从而达到删除的目的“，这就要求我们在prompt的过程中要逻辑清晰，仔细严谨。

如果你的prompt已经出现遗漏或者错误，在中途更改的过程中，我们尽量更改规则则而不是仅仅告诉他出错的这部分要如何处理（授人以鱼不如授人以渔）。例如在本次prompt中，GPT对于新出现单词Diffusion的大小写出现问题，我们可以告诉他”Text-to-Image是一个完整的可发音的单词，所以我们把他改成全部小写，即为text-to-image“（这里使用了CoT）；而不是仅仅告诉他”Text-to-Image需要小写“。这样可以增加模型对于其他新词语的处理能力。

其他方面，CoT 与 Recursive Prompting的混合使用的结合分步骤引导是很有效的（可见1.3.2的小例）。官方的指南中给出的方法已经很丰富且配有例子，但是实际运用也需要更多的练习和对于实际prompt过程中出现问题的理解和思考。

[[Prompt engineering](https://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering)](https://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering)

## 二、总结内容形成PPT和讲解稿

目前生成PPT的在线平台多种多样，我们主要选择体验了一些免费，效果还可以的，并将其介绍如下：

### 1.国内平台，我们以KIMI为例

我们将LLM对项目的总结和演讲稿转为TXT，导入到PPT生成大模型平台。它会自动帮我们分析出大纲。同时，我们可以对大纲进行微调。在生成时，我们要提示大模型生成的PPT章节顺序和内容安排要契合我们的演讲稿。

<img src="md_img\KIMI.png" alt="image-20240731180232970" style="zoom: 25%;" />

然后，我们选择一个模板风格，可以对模板风格进行适当微调。

<img src="md_img\selection.png" alt="image-20240731181036998" style="zoom: 33%;" />

选择完模型之后，我们直接生成PPT，用户还可以自由选择是否对PPT进行微调：

<img src="md_img\adaption.png" alt="image-20240731181218518" style="zoom:33%;" />

微调之后，将生成的PPT下载下来即可。

### 2.国外平台，我们以Canva为例

Canva提供了文稿转为有特色的文档，再由有丰富内容的文档生成PPT的路径。

我们需要先简单选取一个文档模板，将LLM对项目的总结依次填入：

<img src="md_img\canvas.png" alt="image-20240807153825791" style="zoom:50%;" />

这样就得到了一份关于项目的总结，然后使用canva平台的文档转PPT功能，将文档内容生成PPT：

<img src="md_img\trans_into_ppt.png" alt="image-20240808011033517" style="zoom:50%;" />

canva会帮用户生成多组PPT样式，用户挑选PPT后，可以简单调整PPT内容，然后就可以一键下载生成的PPT了：

<img src="md_img\PPT.png" alt="image-20240808011450179" style="zoom:50%;" />

## 三、TTS合成语音讲解

### 1.本地部署语音合成模型

本次项目讲解视频的配音使用的是CosyVoice模型。

CosyVoice语音模型是阿里通义实验室开源的语音模型，它支持自然语音生成，支持多语言、音色和情感控制，在多语言语音生成、零样本语音生成、跨语言声音合成和指令执行能力方面表现卓越。

CosyVoice采用了总共超15万小时的数据训练，支持中英日粤韩5种语言的合成，合成效果显著优于传统语音合成模型。

CosyVoice支持one-shot音色克隆 ，只需要3~10s的原始音频，即可生成模拟音色，甚至包括韵律、情感等细节。在跨语种的语音合成中，也有不俗的表现。

本次作业中，我们使用的CosyVoice语音模型为整合包版：https://www.bilibili.com/video/BV1PE4m1d7JT/

<img src="md_img\cosy_vocie.png" alt="image-20240808011713731" style="zoom:50%;" />

我们可以选择直接使用模型自带的预训练好的音色去读文段，也可以通过上传音色复刻自己喜欢的声音：

<img src="md_img\prompt.png" alt="-" style="zoom:50%;" />

只需上传prompt音频，加上音段的对应文段内容，就可以生成prompt音色读合成文本的音频了。

### 2.免费语音合成平台介绍：

以下是四个免费的语音合成平台，它们都可以将文本转换为高质量的语音输出，非常适合语言学习、内容创作等多种用途：

1. **https://www.text-to-speech.cn**
2. **https://www.ttson.cn**
3. **https://zh-cn.text-to-speech.online**
4. **https://ttsmaker.cn**

这些平台均提供多语言支持和多种声音选项，用户可以根据需求选择合适的语音风格，并可自定义语速、音调和音量，以获得理想的语音效果。它们操作简便、界面友好，无需下载任何软件，即可在线完成文本到语音的转换。

以https://www.text-to-speech.cn为例，进行介绍

#### 使用步骤

1. **访问网站**
   - 打开浏览器，输入网址 https://www.text-to-speech.cn 并进入网站首页。
2. **输入文本**
   - 在首页的文本输入框中，键入或粘贴希望转换为语音的文本内容。
3. **选择语言和声音**
   - 从下拉菜单中选择所需的语言。例如，选择“中文（普通话，简体）”。
   - 选择一个适合的语音风格，如“云希（年轻男）”。
4. **调整质量**
   - 选择音频质量，如“16khz-32kbitrate(mp3)”。
5. **选择模仿（非必选）**
   - 如果需要，可以选择模仿的声音。
6. **选择感情（非必选）**
   - 如果需要，可以选择表达的感情。
7. **设置静音**
   - 默认情况下，系统会在每个句子的结束符号后自动停顿。
8. **调整强度**
   - 默认，弱，强，超强等。
9. **设置音量**
   - 默认音量即可。
10. **调整语速**
    - 默认语速为0，可根据需要进行调整。
11. **调整音调**
    - 默认音调为0，可根据需要进行调整。
12. **生成语音**
    - 完成上述设置后，点击“生成”按钮，平台将开始处理输入的文本并生成语音。
13. **播放和下载**
    - 生成完成后，可以直接在线播放生成的语音，检查其效果。
    - 如果对生成的语音满意，可以点击下载按钮，将语音文件保存到本地，以便离线使用。

#### 示例操作

![image-20240808121921030](C:md_img\20240808121906.png)

假设您有一段文字需要转换为语音，具体操作步骤如下：

1. 访问 https://www.text-to-speech.cn 网站。
2. 在输入框中输入：“你好，欢迎使用我们的文本转语音服务。”
3. 选择“中文（普通话，简体）”和“云希（年轻男）”语音。
4. 选择音频质量为“16khz-32kbitrate(mp3)”。
5. （可选）选择模仿和感情。
6. 使用默认的静音、强度和音量设置。
7. 调整语速和音调为0。
8. 点击“生成”按钮。
9. 播放生成的语音，或点击下载按钮，将语音文件保存到您的电脑上。

通过这些步骤，可以轻松将文本内容转换为高质量的语音，满足各种应用需求。

### 参考目录

[1] [一文读懂「Prompt Engineering」提示词工程-CSDN博客](https://blog.csdn.net/Julialove102123/article/details/135497369)

[2]https://platform.openai.com/docs/guides/prompt-engineering

[3]Kojima T, Gu S S, Reid M, et al. Large language models are zero-shot reasoners[J]. Advances in neural information processing systems, 2022, 35: 22199-22213.https://arxiv.org/abs/2205.11916

[4]Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models[J]. Advances in neural information processing systems, 2022, 35: 24824-24837.https://doi.org/10.48550/arXiv.2201.11903

[5]Wang X, Wei J, Schuurmans D, et al. Self-consistency improves chain of thought reasoning in language models[J]. arXiv preprint arXiv:2203.11171, 2022.https://doi.org/10.48550/arXiv.2203.11171

[6]Yao S, Yu D, Zhao J, et al. Tree of thoughts: Deliberate problem solving with large language models[J]. Advances in Neural Information Processing Systems, 2024, 36.https://doi.org/10.48550/arXiv.2305.10601

[7]Yao S, Zhao J, Yu D, et al. React: Synergizing reasoning and acting in language models[J]. arXiv preprint arXiv:2210.03629, 2022.https://arxiv.org/abs/2210.03629

[8]https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/

[9]Large Language Models Are Human-Level Prompt Engineers[ https://doi.org/10.48550/arXiv.2211.01910](https://doi.org/10.48550/arXiv.2211.01910)

[10]Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling https://doi.org/10.48550/arXiv.2305.09993](https://doi.org/10.48550/arXiv.2305.09993)

[11]Zelikman E, Wu Y, Mu J, et al. Star: Bootstrapping reasoning with reasoning[J]. Advances in Neural Information Processing Systems, 2022, 35: 15476-15488.https://arxiv.org/abs/2203.14465

[12]Shinn N, Cassano F, Gopinath A, et al. Reflexion: Language agents with verbal reinforcement learning[J]. Advances in Neural Information Processing Systems, 2024, 36.https://arxiv.org/abs/2303.11366

[13]Khattab O, Santhanam K, Li X L, et al. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp[J]. arXiv preprint arXiv:2212.14024, 2022.https://arxiv.org/abs/2212.14024

[14]Du Y, Li S, Torralba A, et al. Improving factuality and reasoning in language models through multiagent debate[J]. arXiv preprint arXiv:2305.14325, 2023.[ https://doi.org/10.48550/arXiv.2305.14325](https://doi.org/10.48550/arXiv.2305.14325)

[15]Creswell A, Shanahan M. Faithful reasoning using large language models[J]. arXiv preprint arXiv:2208.14271, 2022.https://arxiv.org/abs/2208.14271

[16]https://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering

[17] [增强语言模型之Recursive prompting-CSDN博客](https://blog.csdn.net/qq_27590277/article/details/129722648)

[18] [Kimi.ai - 帮你看更大的世界 (moonshot.cn)](https://kimi.moonshot.cn/)

[19] [Ollama+Open WebUI本地部署Llama3 8b（附踩坑细节）-CSDN博客](https://blog.csdn.net/qq_53795212/article/details/139690567)

[20] https://www.bilibili.com/video/BV1PE4m1d7JT/